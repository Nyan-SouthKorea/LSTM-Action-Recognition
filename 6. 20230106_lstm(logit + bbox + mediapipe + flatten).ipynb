{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e8e67a",
   "metadata": {},
   "source": [
    "### 저장된 Yolo 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "288762f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\nyan/.cache\\torch\\hub\\ultralytics_yolov5_master\n",
      "YOLOv5  2022-9-15 Python-3.8.15 torch-1.10.1 CUDA:0 (NVIDIA GeForce GTX 1080 Ti, 11264MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 322 layers, 86173414 parameters, 0 gradients\n",
      "Adding AutoShape... \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path = './yolov5-master/exp3/weights/best.pt')\n",
    "def net_yolo(img):\n",
    "    h, w, c = img.shape\n",
    "    pred_list = model(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    pred_list = pred_list.pandas().xyxy[0]\n",
    "    result_list = []\n",
    "    for cnt in range(len(pred_list)):\n",
    "        x1 = int(pred_list.loc[cnt]['xmin'])\n",
    "        y1 = int(pred_list.loc[cnt]['ymin'])\n",
    "        x2 = int(pred_list.loc[cnt]['xmax'])\n",
    "        y2 = int(pred_list.loc[cnt]['ymax'])\n",
    "        x1_nor = max(1, int(pred_list.loc[cnt]['xmin'])) / w\n",
    "        y1_nor = max(1, int(pred_list.loc[cnt]['ymin'])) / h\n",
    "        x2_nor = max(1, int(pred_list.loc[cnt]['xmax'])) / w\n",
    "        y2_nor = max(1, int(pred_list.loc[cnt]['ymax'])) / h\n",
    "        conf = round(float(pred_list.loc[cnt]['confidence']), 3)\n",
    "        pred = {'bbox' : [x1, y1, x2, y2], 'conf' : conf, 'bbox_nor' : [x1_nor, y1_nor, x2_nor, y2_nor]}\n",
    "        result_list.append(pred)\n",
    "    if len(result_list) > 0:\n",
    "        result_list.sort(key = lambda x:x['conf'], reverse = True)\n",
    "        result = result_list[0]                    \n",
    "        crop_img = img[result['bbox'][1]:result['bbox'][3], result['bbox'][0]:result['bbox'][2]]\n",
    "        return result['bbox_nor'], crop_img\n",
    "    else:\n",
    "        return [], []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b091862a",
   "metadata": {},
   "source": [
    "### 저장된 Resnet 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ab25ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "현재 가상환경 GPU 사용 가능상태\n"
     ]
    }
   ],
   "source": [
    "def seed(seed = 1234):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "if torch.cuda.is_available() == True:\n",
    "    device = 'cuda:0'\n",
    "    print('현재 가상환경 GPU 사용 가능상태')\n",
    "else:\n",
    "    device = 'cpu'\n",
    "    print('GPU 사용 불가능 상태')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4ae8e225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv2d-1         [-1, 64, 112, 112]           9,408\n",
      "       BatchNorm2d-2         [-1, 64, 112, 112]             128\n",
      "              ReLU-3         [-1, 64, 112, 112]               0\n",
      "         MaxPool2d-4           [-1, 64, 56, 56]               0\n",
      "            Conv2d-5           [-1, 64, 56, 56]           4,096\n",
      "       BatchNorm2d-6           [-1, 64, 56, 56]             128\n",
      "              ReLU-7           [-1, 64, 56, 56]               0\n",
      "            Conv2d-8           [-1, 64, 56, 56]          36,864\n",
      "       BatchNorm2d-9           [-1, 64, 56, 56]             128\n",
      "             ReLU-10           [-1, 64, 56, 56]               0\n",
      "           Conv2d-11          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-12          [-1, 256, 56, 56]             512\n",
      "           Conv2d-13          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-14          [-1, 256, 56, 56]             512\n",
      "             ReLU-15          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-16          [-1, 256, 56, 56]               0\n",
      "           Conv2d-17           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-18           [-1, 64, 56, 56]             128\n",
      "             ReLU-19           [-1, 64, 56, 56]               0\n",
      "           Conv2d-20           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-21           [-1, 64, 56, 56]             128\n",
      "             ReLU-22           [-1, 64, 56, 56]               0\n",
      "           Conv2d-23          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-24          [-1, 256, 56, 56]             512\n",
      "             ReLU-25          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-26          [-1, 256, 56, 56]               0\n",
      "           Conv2d-27           [-1, 64, 56, 56]          16,384\n",
      "      BatchNorm2d-28           [-1, 64, 56, 56]             128\n",
      "             ReLU-29           [-1, 64, 56, 56]               0\n",
      "           Conv2d-30           [-1, 64, 56, 56]          36,864\n",
      "      BatchNorm2d-31           [-1, 64, 56, 56]             128\n",
      "             ReLU-32           [-1, 64, 56, 56]               0\n",
      "           Conv2d-33          [-1, 256, 56, 56]          16,384\n",
      "      BatchNorm2d-34          [-1, 256, 56, 56]             512\n",
      "             ReLU-35          [-1, 256, 56, 56]               0\n",
      "       Bottleneck-36          [-1, 256, 56, 56]               0\n",
      "           Conv2d-37          [-1, 128, 56, 56]          32,768\n",
      "      BatchNorm2d-38          [-1, 128, 56, 56]             256\n",
      "             ReLU-39          [-1, 128, 56, 56]               0\n",
      "           Conv2d-40          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-41          [-1, 128, 28, 28]             256\n",
      "             ReLU-42          [-1, 128, 28, 28]               0\n",
      "           Conv2d-43          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-44          [-1, 512, 28, 28]           1,024\n",
      "           Conv2d-45          [-1, 512, 28, 28]         131,072\n",
      "      BatchNorm2d-46          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-47          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-48          [-1, 512, 28, 28]               0\n",
      "           Conv2d-49          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-50          [-1, 128, 28, 28]             256\n",
      "             ReLU-51          [-1, 128, 28, 28]               0\n",
      "           Conv2d-52          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-53          [-1, 128, 28, 28]             256\n",
      "             ReLU-54          [-1, 128, 28, 28]               0\n",
      "           Conv2d-55          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-56          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-57          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-58          [-1, 512, 28, 28]               0\n",
      "           Conv2d-59          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-60          [-1, 128, 28, 28]             256\n",
      "             ReLU-61          [-1, 128, 28, 28]               0\n",
      "           Conv2d-62          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-63          [-1, 128, 28, 28]             256\n",
      "             ReLU-64          [-1, 128, 28, 28]               0\n",
      "           Conv2d-65          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-66          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-67          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-68          [-1, 512, 28, 28]               0\n",
      "           Conv2d-69          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-70          [-1, 128, 28, 28]             256\n",
      "             ReLU-71          [-1, 128, 28, 28]               0\n",
      "           Conv2d-72          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-73          [-1, 128, 28, 28]             256\n",
      "             ReLU-74          [-1, 128, 28, 28]               0\n",
      "           Conv2d-75          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-76          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-77          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-78          [-1, 512, 28, 28]               0\n",
      "           Conv2d-79          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-80          [-1, 128, 28, 28]             256\n",
      "             ReLU-81          [-1, 128, 28, 28]               0\n",
      "           Conv2d-82          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-83          [-1, 128, 28, 28]             256\n",
      "             ReLU-84          [-1, 128, 28, 28]               0\n",
      "           Conv2d-85          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-86          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-87          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-88          [-1, 512, 28, 28]               0\n",
      "           Conv2d-89          [-1, 128, 28, 28]          65,536\n",
      "      BatchNorm2d-90          [-1, 128, 28, 28]             256\n",
      "             ReLU-91          [-1, 128, 28, 28]               0\n",
      "           Conv2d-92          [-1, 128, 28, 28]         147,456\n",
      "      BatchNorm2d-93          [-1, 128, 28, 28]             256\n",
      "             ReLU-94          [-1, 128, 28, 28]               0\n",
      "           Conv2d-95          [-1, 512, 28, 28]          65,536\n",
      "      BatchNorm2d-96          [-1, 512, 28, 28]           1,024\n",
      "             ReLU-97          [-1, 512, 28, 28]               0\n",
      "       Bottleneck-98          [-1, 512, 28, 28]               0\n",
      "           Conv2d-99          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-100          [-1, 128, 28, 28]             256\n",
      "            ReLU-101          [-1, 128, 28, 28]               0\n",
      "          Conv2d-102          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-103          [-1, 128, 28, 28]             256\n",
      "            ReLU-104          [-1, 128, 28, 28]               0\n",
      "          Conv2d-105          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-106          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-107          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-108          [-1, 512, 28, 28]               0\n",
      "          Conv2d-109          [-1, 128, 28, 28]          65,536\n",
      "     BatchNorm2d-110          [-1, 128, 28, 28]             256\n",
      "            ReLU-111          [-1, 128, 28, 28]               0\n",
      "          Conv2d-112          [-1, 128, 28, 28]         147,456\n",
      "     BatchNorm2d-113          [-1, 128, 28, 28]             256\n",
      "            ReLU-114          [-1, 128, 28, 28]               0\n",
      "          Conv2d-115          [-1, 512, 28, 28]          65,536\n",
      "     BatchNorm2d-116          [-1, 512, 28, 28]           1,024\n",
      "            ReLU-117          [-1, 512, 28, 28]               0\n",
      "      Bottleneck-118          [-1, 512, 28, 28]               0\n",
      "          Conv2d-119          [-1, 256, 28, 28]         131,072\n",
      "     BatchNorm2d-120          [-1, 256, 28, 28]             512\n",
      "            ReLU-121          [-1, 256, 28, 28]               0\n",
      "          Conv2d-122          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-123          [-1, 256, 14, 14]             512\n",
      "            ReLU-124          [-1, 256, 14, 14]               0\n",
      "          Conv2d-125         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-126         [-1, 1024, 14, 14]           2,048\n",
      "          Conv2d-127         [-1, 1024, 14, 14]         524,288\n",
      "     BatchNorm2d-128         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-129         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-130         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-131          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-132          [-1, 256, 14, 14]             512\n",
      "            ReLU-133          [-1, 256, 14, 14]               0\n",
      "          Conv2d-134          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-135          [-1, 256, 14, 14]             512\n",
      "            ReLU-136          [-1, 256, 14, 14]               0\n",
      "          Conv2d-137         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-138         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-139         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-140         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-141          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-142          [-1, 256, 14, 14]             512\n",
      "            ReLU-143          [-1, 256, 14, 14]               0\n",
      "          Conv2d-144          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-145          [-1, 256, 14, 14]             512\n",
      "            ReLU-146          [-1, 256, 14, 14]               0\n",
      "          Conv2d-147         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-148         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-149         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-150         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-151          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-152          [-1, 256, 14, 14]             512\n",
      "            ReLU-153          [-1, 256, 14, 14]               0\n",
      "          Conv2d-154          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-155          [-1, 256, 14, 14]             512\n",
      "            ReLU-156          [-1, 256, 14, 14]               0\n",
      "          Conv2d-157         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-158         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-159         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-160         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-161          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-162          [-1, 256, 14, 14]             512\n",
      "            ReLU-163          [-1, 256, 14, 14]               0\n",
      "          Conv2d-164          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-165          [-1, 256, 14, 14]             512\n",
      "            ReLU-166          [-1, 256, 14, 14]               0\n",
      "          Conv2d-167         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-168         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-169         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-170         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-171          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-172          [-1, 256, 14, 14]             512\n",
      "            ReLU-173          [-1, 256, 14, 14]               0\n",
      "          Conv2d-174          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-175          [-1, 256, 14, 14]             512\n",
      "            ReLU-176          [-1, 256, 14, 14]               0\n",
      "          Conv2d-177         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-178         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-179         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-180         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-181          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-182          [-1, 256, 14, 14]             512\n",
      "            ReLU-183          [-1, 256, 14, 14]               0\n",
      "          Conv2d-184          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-185          [-1, 256, 14, 14]             512\n",
      "            ReLU-186          [-1, 256, 14, 14]               0\n",
      "          Conv2d-187         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-188         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-189         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-190         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-191          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-192          [-1, 256, 14, 14]             512\n",
      "            ReLU-193          [-1, 256, 14, 14]               0\n",
      "          Conv2d-194          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-195          [-1, 256, 14, 14]             512\n",
      "            ReLU-196          [-1, 256, 14, 14]               0\n",
      "          Conv2d-197         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-198         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-199         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-200         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-201          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-202          [-1, 256, 14, 14]             512\n",
      "            ReLU-203          [-1, 256, 14, 14]               0\n",
      "          Conv2d-204          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-205          [-1, 256, 14, 14]             512\n",
      "            ReLU-206          [-1, 256, 14, 14]               0\n",
      "          Conv2d-207         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-208         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-209         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-210         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-211          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-212          [-1, 256, 14, 14]             512\n",
      "            ReLU-213          [-1, 256, 14, 14]               0\n",
      "          Conv2d-214          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-215          [-1, 256, 14, 14]             512\n",
      "            ReLU-216          [-1, 256, 14, 14]               0\n",
      "          Conv2d-217         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-218         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-219         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-220         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-221          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-222          [-1, 256, 14, 14]             512\n",
      "            ReLU-223          [-1, 256, 14, 14]               0\n",
      "          Conv2d-224          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-225          [-1, 256, 14, 14]             512\n",
      "            ReLU-226          [-1, 256, 14, 14]               0\n",
      "          Conv2d-227         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-228         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-229         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-230         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-231          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-232          [-1, 256, 14, 14]             512\n",
      "            ReLU-233          [-1, 256, 14, 14]               0\n",
      "          Conv2d-234          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-235          [-1, 256, 14, 14]             512\n",
      "            ReLU-236          [-1, 256, 14, 14]               0\n",
      "          Conv2d-237         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-238         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-239         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-240         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-241          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-242          [-1, 256, 14, 14]             512\n",
      "            ReLU-243          [-1, 256, 14, 14]               0\n",
      "          Conv2d-244          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-245          [-1, 256, 14, 14]             512\n",
      "            ReLU-246          [-1, 256, 14, 14]               0\n",
      "          Conv2d-247         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-248         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-249         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-250         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-251          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-252          [-1, 256, 14, 14]             512\n",
      "            ReLU-253          [-1, 256, 14, 14]               0\n",
      "          Conv2d-254          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-255          [-1, 256, 14, 14]             512\n",
      "            ReLU-256          [-1, 256, 14, 14]               0\n",
      "          Conv2d-257         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-258         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-259         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-260         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-261          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-262          [-1, 256, 14, 14]             512\n",
      "            ReLU-263          [-1, 256, 14, 14]               0\n",
      "          Conv2d-264          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-265          [-1, 256, 14, 14]             512\n",
      "            ReLU-266          [-1, 256, 14, 14]               0\n",
      "          Conv2d-267         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-268         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-269         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-270         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-271          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-272          [-1, 256, 14, 14]             512\n",
      "            ReLU-273          [-1, 256, 14, 14]               0\n",
      "          Conv2d-274          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-275          [-1, 256, 14, 14]             512\n",
      "            ReLU-276          [-1, 256, 14, 14]               0\n",
      "          Conv2d-277         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-278         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-279         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-280         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-281          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-282          [-1, 256, 14, 14]             512\n",
      "            ReLU-283          [-1, 256, 14, 14]               0\n",
      "          Conv2d-284          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-285          [-1, 256, 14, 14]             512\n",
      "            ReLU-286          [-1, 256, 14, 14]               0\n",
      "          Conv2d-287         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-288         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-289         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-290         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-291          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-292          [-1, 256, 14, 14]             512\n",
      "            ReLU-293          [-1, 256, 14, 14]               0\n",
      "          Conv2d-294          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-295          [-1, 256, 14, 14]             512\n",
      "            ReLU-296          [-1, 256, 14, 14]               0\n",
      "          Conv2d-297         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-298         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-299         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-300         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-301          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-302          [-1, 256, 14, 14]             512\n",
      "            ReLU-303          [-1, 256, 14, 14]               0\n",
      "          Conv2d-304          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-305          [-1, 256, 14, 14]             512\n",
      "            ReLU-306          [-1, 256, 14, 14]               0\n",
      "          Conv2d-307         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-308         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-309         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-310         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-311          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-312          [-1, 256, 14, 14]             512\n",
      "            ReLU-313          [-1, 256, 14, 14]               0\n",
      "          Conv2d-314          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-315          [-1, 256, 14, 14]             512\n",
      "            ReLU-316          [-1, 256, 14, 14]               0\n",
      "          Conv2d-317         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-318         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-319         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-320         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-321          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-322          [-1, 256, 14, 14]             512\n",
      "            ReLU-323          [-1, 256, 14, 14]               0\n",
      "          Conv2d-324          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-325          [-1, 256, 14, 14]             512\n",
      "            ReLU-326          [-1, 256, 14, 14]               0\n",
      "          Conv2d-327         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-328         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-329         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-330         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-331          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-332          [-1, 256, 14, 14]             512\n",
      "            ReLU-333          [-1, 256, 14, 14]               0\n",
      "          Conv2d-334          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-335          [-1, 256, 14, 14]             512\n",
      "            ReLU-336          [-1, 256, 14, 14]               0\n",
      "          Conv2d-337         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-338         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-339         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-340         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-341          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-342          [-1, 256, 14, 14]             512\n",
      "            ReLU-343          [-1, 256, 14, 14]               0\n",
      "          Conv2d-344          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-345          [-1, 256, 14, 14]             512\n",
      "            ReLU-346          [-1, 256, 14, 14]               0\n",
      "          Conv2d-347         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-348         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-349         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-350         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-351          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-352          [-1, 256, 14, 14]             512\n",
      "            ReLU-353          [-1, 256, 14, 14]               0\n",
      "          Conv2d-354          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-355          [-1, 256, 14, 14]             512\n",
      "            ReLU-356          [-1, 256, 14, 14]               0\n",
      "          Conv2d-357         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-358         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-359         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-360         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-361          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-362          [-1, 256, 14, 14]             512\n",
      "            ReLU-363          [-1, 256, 14, 14]               0\n",
      "          Conv2d-364          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-365          [-1, 256, 14, 14]             512\n",
      "            ReLU-366          [-1, 256, 14, 14]               0\n",
      "          Conv2d-367         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-368         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-369         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-370         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-371          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-372          [-1, 256, 14, 14]             512\n",
      "            ReLU-373          [-1, 256, 14, 14]               0\n",
      "          Conv2d-374          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-375          [-1, 256, 14, 14]             512\n",
      "            ReLU-376          [-1, 256, 14, 14]               0\n",
      "          Conv2d-377         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-378         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-379         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-380         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-381          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-382          [-1, 256, 14, 14]             512\n",
      "            ReLU-383          [-1, 256, 14, 14]               0\n",
      "          Conv2d-384          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-385          [-1, 256, 14, 14]             512\n",
      "            ReLU-386          [-1, 256, 14, 14]               0\n",
      "          Conv2d-387         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-388         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-389         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-390         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-391          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-392          [-1, 256, 14, 14]             512\n",
      "            ReLU-393          [-1, 256, 14, 14]               0\n",
      "          Conv2d-394          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-395          [-1, 256, 14, 14]             512\n",
      "            ReLU-396          [-1, 256, 14, 14]               0\n",
      "          Conv2d-397         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-398         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-399         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-400         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-401          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-402          [-1, 256, 14, 14]             512\n",
      "            ReLU-403          [-1, 256, 14, 14]               0\n",
      "          Conv2d-404          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-405          [-1, 256, 14, 14]             512\n",
      "            ReLU-406          [-1, 256, 14, 14]               0\n",
      "          Conv2d-407         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-408         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-409         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-410         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-411          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-412          [-1, 256, 14, 14]             512\n",
      "            ReLU-413          [-1, 256, 14, 14]               0\n",
      "          Conv2d-414          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-415          [-1, 256, 14, 14]             512\n",
      "            ReLU-416          [-1, 256, 14, 14]               0\n",
      "          Conv2d-417         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-418         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-419         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-420         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-421          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-422          [-1, 256, 14, 14]             512\n",
      "            ReLU-423          [-1, 256, 14, 14]               0\n",
      "          Conv2d-424          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-425          [-1, 256, 14, 14]             512\n",
      "            ReLU-426          [-1, 256, 14, 14]               0\n",
      "          Conv2d-427         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-428         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-429         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-430         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-431          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-432          [-1, 256, 14, 14]             512\n",
      "            ReLU-433          [-1, 256, 14, 14]               0\n",
      "          Conv2d-434          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-435          [-1, 256, 14, 14]             512\n",
      "            ReLU-436          [-1, 256, 14, 14]               0\n",
      "          Conv2d-437         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-438         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-439         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-440         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-441          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-442          [-1, 256, 14, 14]             512\n",
      "            ReLU-443          [-1, 256, 14, 14]               0\n",
      "          Conv2d-444          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-445          [-1, 256, 14, 14]             512\n",
      "            ReLU-446          [-1, 256, 14, 14]               0\n",
      "          Conv2d-447         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-448         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-449         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-450         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-451          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-452          [-1, 256, 14, 14]             512\n",
      "            ReLU-453          [-1, 256, 14, 14]               0\n",
      "          Conv2d-454          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-455          [-1, 256, 14, 14]             512\n",
      "            ReLU-456          [-1, 256, 14, 14]               0\n",
      "          Conv2d-457         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-458         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-459         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-460         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-461          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-462          [-1, 256, 14, 14]             512\n",
      "            ReLU-463          [-1, 256, 14, 14]               0\n",
      "          Conv2d-464          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-465          [-1, 256, 14, 14]             512\n",
      "            ReLU-466          [-1, 256, 14, 14]               0\n",
      "          Conv2d-467         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-468         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-469         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-470         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-471          [-1, 256, 14, 14]         262,144\n",
      "     BatchNorm2d-472          [-1, 256, 14, 14]             512\n",
      "            ReLU-473          [-1, 256, 14, 14]               0\n",
      "          Conv2d-474          [-1, 256, 14, 14]         589,824\n",
      "     BatchNorm2d-475          [-1, 256, 14, 14]             512\n",
      "            ReLU-476          [-1, 256, 14, 14]               0\n",
      "          Conv2d-477         [-1, 1024, 14, 14]         262,144\n",
      "     BatchNorm2d-478         [-1, 1024, 14, 14]           2,048\n",
      "            ReLU-479         [-1, 1024, 14, 14]               0\n",
      "      Bottleneck-480         [-1, 1024, 14, 14]               0\n",
      "          Conv2d-481          [-1, 512, 14, 14]         524,288\n",
      "     BatchNorm2d-482          [-1, 512, 14, 14]           1,024\n",
      "            ReLU-483          [-1, 512, 14, 14]               0\n",
      "          Conv2d-484            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-485            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-486            [-1, 512, 7, 7]               0\n",
      "          Conv2d-487           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-488           [-1, 2048, 7, 7]           4,096\n",
      "          Conv2d-489           [-1, 2048, 7, 7]       2,097,152\n",
      "     BatchNorm2d-490           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-491           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-492           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-493            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-494            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-495            [-1, 512, 7, 7]               0\n",
      "          Conv2d-496            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-497            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-498            [-1, 512, 7, 7]               0\n",
      "          Conv2d-499           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-500           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-501           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-502           [-1, 2048, 7, 7]               0\n",
      "          Conv2d-503            [-1, 512, 7, 7]       1,048,576\n",
      "     BatchNorm2d-504            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-505            [-1, 512, 7, 7]               0\n",
      "          Conv2d-506            [-1, 512, 7, 7]       2,359,296\n",
      "     BatchNorm2d-507            [-1, 512, 7, 7]           1,024\n",
      "            ReLU-508            [-1, 512, 7, 7]               0\n",
      "          Conv2d-509           [-1, 2048, 7, 7]       1,048,576\n",
      "     BatchNorm2d-510           [-1, 2048, 7, 7]           4,096\n",
      "            ReLU-511           [-1, 2048, 7, 7]               0\n",
      "      Bottleneck-512           [-1, 2048, 7, 7]               0\n",
      "AdaptiveAvgPool2d-513           [-1, 2048, 1, 1]               0\n",
      "          Linear-514                    [-1, 4]           8,196\n",
      "================================================================\n",
      "Total params: 58,152,004\n",
      "Trainable params: 58,152,004\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.57\n",
      "Forward/backward pass size (MB): 606.58\n",
      "Params size (MB): 221.83\n",
      "Estimated Total Size (MB): 828.99\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchvision import models\n",
    "import torch.nn as nn\n",
    "from torchsummary import summary as Summary\n",
    "\n",
    "seed()\n",
    "model_resnet = models.resnet152(pretrained = True)\n",
    "num_ftrs = model_resnet.fc.in_features\n",
    "model_resnet.fc = nn.Linear(num_ftrs, 4)\n",
    "model_resnet.load_state_dict(torch.load('./best_model_cnn.pth'))\n",
    "net_cnn = model_resnet.to(device)\n",
    "net_cnn.eval()\n",
    "Summary(net_cnn.to(device), (3, 224, 224))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0ab583",
   "metadata": {},
   "source": [
    "### MediaPipe 모델 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e6f168db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_hands = mp.solutions.hands\n",
    "hands =  mp_hands.Hands(static_image_mode=True, max_num_hands=1, min_detection_confidence=0.3)\n",
    "def net_mediapipe(img):\n",
    "    results = hands.process(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
    "    if not results.multi_hand_landmarks: return []\n",
    "    xy_list = []\n",
    "    for one_hand in results.multi_hand_landmarks:\n",
    "        for xy in one_hand.landmark:\n",
    "            xy_list.append(xy.x)\n",
    "            xy_list.append(xy.y)     \n",
    "    return xy_list # 42개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c44a036",
   "metadata": {},
   "source": [
    "### 저장된 LSTM 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb8c5a8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "skeleton_LSTM(\n",
       "  (lstm1): LSTM(3186, 4096, batch_first=True)\n",
       "  (batch1): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm2): LSTM(4096, 2048, batch_first=True)\n",
       "  (batch2): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm3): LSTM(2048, 1024, batch_first=True)\n",
       "  (batch3): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout1): Dropout(p=0.3, inplace=False)\n",
       "  (lstm4): LSTM(1024, 512, batch_first=True)\n",
       "  (batch4): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm5): LSTM(512, 128, batch_first=True)\n",
       "  (batch5): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (lstm6): LSTM(128, 64, batch_first=True)\n",
       "  (batch6): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (dropout2): Dropout(p=0.3, inplace=False)\n",
       "  (lstm7): LSTM(64, 32, batch_first=True)\n",
       "  (batch7): BatchNorm1d(30, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc): Linear(in_features=32, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class skeleton_LSTM(nn.Module) :\n",
    "    def __init__(self):\n",
    "        super(skeleton_LSTM, self).__init__()\n",
    "        self.lstm1 = nn.LSTM(input_size=3186, hidden_size=4096, num_layers=1, batch_first=True)\n",
    "        self.batch1 = nn.BatchNorm1d(30)\n",
    "        self.lstm2 = nn.LSTM(input_size=4096, hidden_size=2048, num_layers=1, batch_first=True)\n",
    "        self.batch2 = nn.BatchNorm1d(30)\n",
    "        self.lstm3 = nn.LSTM(input_size=2048, hidden_size=1024, num_layers=1, batch_first=True)\n",
    "        self.batch3 = nn.BatchNorm1d(30)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.lstm4 = nn.LSTM(input_size=1024, hidden_size=512, num_layers=1, batch_first=True)\n",
    "        self.batch4 = nn.BatchNorm1d(30)\n",
    "        self.lstm5 = nn.LSTM(input_size=512, hidden_size=128, num_layers=1, batch_first=True)\n",
    "        self.batch5 = nn.BatchNorm1d(30)\n",
    "        self.lstm6 = nn.LSTM(input_size=128, hidden_size=64, num_layers=1, batch_first=True)\n",
    "        self.batch6 = nn.BatchNorm1d(30)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.lstm7 = nn.LSTM(input_size=64, hidden_size=32, num_layers=1, batch_first=True)\n",
    "        self.batch7 = nn.BatchNorm1d(30)\n",
    "        self.fc = nn.Linear(32,5)\n",
    "    def forward(self, x) :\n",
    "        x, _ = self.lstm1(x)\n",
    "        x = self.batch1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = self.batch2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x = self.batch3(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        x = self.batch4(x)\n",
    "        x, _ = self.lstm5(x)\n",
    "        x = self.batch5(x)\n",
    "        x, _ = self.lstm6(x)\n",
    "        x = self.batch6(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm7(x)\n",
    "        x = self.batch7(x)\n",
    "        x = self.fc(x[:,-1,:])\n",
    "        return x\n",
    "    \n",
    "net_lstm = skeleton_LSTM()\n",
    "net_lstm.load_state_dict(torch.load('./model_lstm_yolo_cnn_mediapipe_flatten_wholetrain.pth'))\n",
    "net_lstm = net_lstm.to(device)\n",
    "net_lstm.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00eb5598",
   "metadata": {},
   "source": [
    "### 레이블링 된 Test 데이터 돌려보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27bd944b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.transforms import ToTensor, Resize, Normalize\n",
    "import numpy as np\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import cv2\n",
    "import natsort\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import minmax_scale\n",
    "import gc\n",
    "from torch.cuda import memory_allocated, empty_cache\n",
    "\n",
    "seed()\n",
    "mean = np.array([0.485, 0.456, 0.406])\n",
    "std = np.array([0.229, 0.224, 0.225])\n",
    "data_transforms = transforms.Compose([ToTensor(), Resize((224,224)), Normalize(mean, std)])\n",
    "class dataset_cnn(Dataset):\n",
    "    def __init__(self, img, label):\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = data_transforms(img)\n",
    "        self.img = [img]\n",
    "        self.label = [label]\n",
    "    def __getitem__(self, index):\n",
    "        data = self.img[index]\n",
    "        label = self.label[index]\n",
    "        return data, label\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    \n",
    "class dataset_lstm(Dataset):\n",
    "    def __init__(self, seq_list):\n",
    "        self.X = []\n",
    "        self.y = []\n",
    "        for dic in seq_list :\n",
    "            self.y.append(dic['key'])\n",
    "            self.X.append(dic['value'])\n",
    "    def __getitem__(self, index):\n",
    "        data = self.X[index]\n",
    "        label = self.y[index]\n",
    "        return torch.Tensor(np.array(data)), torch.tensor(np.array(int(label)))\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "def clear_memory():\n",
    "    if device != 'cpu':\n",
    "        empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def list_augmentation(input_list, goal_len):\n",
    "    need_aug = goal_len - len(input_list)\n",
    "    aug_tempo = need_aug / len(input_list)\n",
    "    full_list, cnt = [], 0\n",
    "    for _input in input_list:\n",
    "        cnt += aug_tempo\n",
    "        full_list.append(_input)\n",
    "        while True:\n",
    "            if cnt < 1: break\n",
    "            full_list.append(_input)\n",
    "            cnt -= 1\n",
    "    while True:\n",
    "        if len(full_list) == goal_len: break\n",
    "        if len(full_list) > goal_len: del full_list[-1]\n",
    "        else: full_list.append(full_list[-1])    \n",
    "    return full_list\n",
    "\n",
    "def final_model(video_path):\n",
    "    last_mp_result = []\n",
    "    for i in range(42):\n",
    "        last_mp_result.append(0.0)\n",
    "    logit_bbox_mp_list = []\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if cap.isOpened():\n",
    "        while True:\n",
    "            ret, img = cap.read()\n",
    "            if ret == True:\n",
    "                bbox, crop_img = net_yolo(img)\n",
    "                if len(crop_img) == 0: continue\n",
    "                flatten_img = cv2.resize(crop_img, (56, 56))\n",
    "                flatten_img = cv2.cvtColor(flatten_img, cv2.COLOR_BGR2GRAY)\n",
    "                flatten_img = flatten_img.reshape(56*56)\n",
    "                flatten_img = list(minmax_scale(flatten_img, axis=0, copy=True))\n",
    "                dataset = dataset_cnn(crop_img, 0)\n",
    "                dataset = DataLoader(dataset)\n",
    "                for data, label in dataset:\n",
    "                    data = data.to(device)\n",
    "                    with torch.no_grad():\n",
    "                        cnn_result = net_cnn(data)\n",
    "                    cnn_logits = []\n",
    "                    for logit in cnn_result[0]:\n",
    "                        cnn_logits.append(logit.item())\n",
    "                    mp_result = net_mediapipe(img)\n",
    "                    if len(mp_result) == 0: mp_result = last_mp_result + []\n",
    "                    last_mp_result = mp_result + []\n",
    "                    logit_bbox_mp_flatten = cnn_logits + bbox + last_mp_result + flatten_img\n",
    "                    logit_bbox_mp_list.append(logit_bbox_mp_flatten)\n",
    "            else: break\n",
    "    logit_bbox_mp_list = list_augmentation(logit_bbox_mp_list, 30)\n",
    "    dataset_dict_list = [{'key':0, 'value':logit_bbox_mp_list}]\n",
    "    dataset = dataset_lstm(dataset_dict_list)\n",
    "    dataset = DataLoader(dataset)\n",
    "    for data, label in dataset:\n",
    "        data = data.to(device)\n",
    "        with torch.no_grad():\n",
    "            result = net_lstm(data)\n",
    "        _, out = torch.max(result, 1)\n",
    "        pred_class = out.item()\n",
    "    return pred_class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9282d44",
   "metadata": {},
   "source": [
    "### 데이콘 제출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a1f25dc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 153/153 [09:00<00:00,  3.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csv 저장 완료\n",
      "TEST_000 | GT: 1 | Pred: 1 | TP\n",
      "TEST_001 | GT: 3 | Pred: 3 | TP\n",
      "TEST_002 | GT: 0 | Pred: 0 | TP\n",
      "TEST_003 | GT: 2 | Pred: 2 | TP\n",
      "TEST_004 | GT: 4 | Pred: 4 | TP\n",
      "TEST_005 | GT: 2 | Pred: 2 | TP\n",
      "TEST_006 | GT: 4 | Pred: 4 | TP\n",
      "TEST_007 | GT: 3 | Pred: 3 | TP\n",
      "TEST_008 | GT: 1 | Pred: 1 | TP\n",
      "TEST_009 | GT: 2 | Pred: 2 | TP\n",
      "TEST_010 | GT: 2 | Pred: 2 | TP\n",
      "TEST_011 | GT: 2 | Pred: 2 | TP\n",
      "TEST_012 | GT: 0 | Pred: 0 | TP\n",
      "TEST_013 | GT: 4 | Pred: 4 | TP\n",
      "TEST_014 | GT: 1 | Pred: 1 | TP\n",
      "TEST_015 | GT: 4 | Pred: 4 | TP\n",
      "TEST_016 | GT: 3 | Pred: 3 | TP\n",
      "TEST_017 | GT: 1 | Pred: 1 | TP\n",
      "TEST_018 | GT: 4 | Pred: 4 | TP\n",
      "TEST_019 | GT: 1 | Pred: 1 | TP\n",
      "TEST_020 | GT: 2 | Pred: 2 | TP\n",
      "TEST_021 | GT: 2 | Pred: 2 | TP\n",
      "TEST_022 | GT: 2 | Pred: 2 | TP\n",
      "TEST_023 | GT: 3 | Pred: 3 | TP\n",
      "TEST_024 | GT: 2 | Pred: 2 | TP\n",
      "TEST_025 | GT: 3 | Pred: 3 | TP\n",
      "TEST_026 | GT: 1 | Pred: 1 | TP\n",
      "TEST_027 | GT: 4 | Pred: 4 | TP\n",
      "TEST_028 | GT: 4 | Pred: 4 | TP\n",
      "TEST_029 | GT: 1 | Pred: 1 | TP\n",
      "TEST_030 | GT: 3 | Pred: 3 | TP\n",
      "TEST_031 | GT: 1 | Pred: 1 | TP\n",
      "TEST_032 | GT: 0 | Pred: 0 | TP\n",
      "TEST_033 | GT: 4 | Pred: 4 | TP\n",
      "TEST_034 | GT: 3 | Pred: 2 | FP <-- 틀림\n",
      "TEST_035 | GT: 4 | Pred: 4 | TP\n",
      "TEST_036 | GT: 2 | Pred: 2 | TP\n",
      "TEST_037 | GT: 3 | Pred: 3 | TP\n",
      "TEST_038 | GT: 2 | Pred: 2 | TP\n",
      "TEST_039 | GT: 0 | Pred: 0 | TP\n",
      "TEST_040 | GT: 2 | Pred: 2 | TP\n",
      "TEST_041 | GT: 4 | Pred: 4 | TP\n",
      "TEST_042 | GT: 4 | Pred: 4 | TP\n",
      "TEST_043 | GT: 3 | Pred: 3 | TP\n",
      "TEST_044 | GT: 3 | Pred: 3 | TP\n",
      "TEST_045 | GT: 4 | Pred: 4 | TP\n",
      "TEST_046 | GT: 4 | Pred: 4 | TP\n",
      "TEST_047 | GT: 0 | Pred: 0 | TP\n",
      "TEST_048 | GT: 4 | Pred: 4 | TP\n",
      "TEST_049 | GT: 1 | Pred: 1 | TP\n",
      "TEST_050 | GT: 3 | Pred: 3 | TP\n",
      "TEST_051 | GT: 4 | Pred: 4 | TP\n",
      "TEST_052 | GT: 4 | Pred: 4 | TP\n",
      "TEST_053 | GT: 0 | Pred: 0 | TP\n",
      "TEST_054 | GT: 1 | Pred: 1 | TP\n",
      "TEST_055 | GT: 3 | Pred: 3 | TP\n",
      "TEST_056 | GT: 4 | Pred: 4 | TP\n",
      "TEST_057 | GT: 2 | Pred: 2 | TP\n",
      "TEST_058 | GT: 0 | Pred: 0 | TP\n",
      "TEST_059 | GT: 4 | Pred: 4 | TP\n",
      "TEST_060 | GT: 0 | Pred: 0 | TP\n",
      "TEST_061 | GT: 3 | Pred: 3 | TP\n",
      "TEST_062 | GT: 4 | Pred: 4 | TP\n",
      "TEST_063 | GT: 2 | Pred: 2 | TP\n",
      "TEST_064 | GT: 4 | Pred: 4 | TP\n",
      "TEST_065 | GT: 2 | Pred: 2 | TP\n",
      "TEST_066 | GT: 1 | Pred: 1 | TP\n",
      "TEST_067 | GT: 0 | Pred: 0 | TP\n",
      "TEST_068 | GT: 2 | Pred: 2 | TP\n",
      "TEST_069 | GT: 4 | Pred: 4 | TP\n",
      "TEST_070 | GT: 3 | Pred: 3 | TP\n",
      "TEST_071 | GT: 1 | Pred: 1 | TP\n",
      "TEST_072 | GT: 3 | Pred: 3 | TP\n",
      "TEST_073 | GT: 3 | Pred: 2 | FP <-- 틀림\n",
      "TEST_074 | GT: 3 | Pred: 3 | TP\n",
      "TEST_075 | GT: 0 | Pred: 0 | TP\n",
      "TEST_076 | GT: 4 | Pred: 4 | TP\n",
      "TEST_077 | GT: 4 | Pred: 4 | TP\n",
      "TEST_078 | GT: 3 | Pred: 3 | TP\n",
      "TEST_079 | GT: 0 | Pred: 0 | TP\n",
      "TEST_080 | GT: 1 | Pred: 1 | TP\n",
      "TEST_081 | GT: 4 | Pred: 4 | TP\n",
      "TEST_082 | GT: 2 | Pred: 3 | FP <-- 틀림\n",
      "TEST_083 | GT: 1 | Pred: 1 | TP\n",
      "TEST_084 | GT: 3 | Pred: 2 | FP <-- 틀림\n",
      "TEST_085 | GT: 2 | Pred: 2 | TP\n",
      "TEST_086 | GT: 4 | Pred: 4 | TP\n",
      "TEST_087 | GT: 3 | Pred: 3 | TP\n",
      "TEST_088 | GT: 0 | Pred: 0 | TP\n",
      "TEST_089 | GT: 2 | Pred: 2 | TP\n",
      "TEST_090 | GT: 1 | Pred: 1 | TP\n",
      "TEST_091 | GT: 2 | Pred: 2 | TP\n",
      "TEST_092 | GT: 2 | Pred: 2 | TP\n",
      "TEST_093 | GT: 0 | Pred: 0 | TP\n",
      "TEST_094 | GT: 4 | Pred: 4 | TP\n",
      "TEST_095 | GT: 1 | Pred: 1 | TP\n",
      "TEST_096 | GT: 4 | Pred: 4 | TP\n",
      "TEST_097 | GT: 0 | Pred: 0 | TP\n",
      "TEST_098 | GT: 2 | Pred: 2 | TP\n",
      "TEST_099 | GT: 3 | Pred: 3 | TP\n",
      "TEST_100 | GT: 2 | Pred: 2 | TP\n",
      "TEST_101 | GT: 4 | Pred: 4 | TP\n",
      "TEST_102 | GT: 1 | Pred: 1 | TP\n",
      "TEST_103 | GT: 4 | Pred: 4 | TP\n",
      "TEST_104 | GT: 0 | Pred: 0 | TP\n",
      "TEST_105 | GT: 2 | Pred: 2 | TP\n",
      "TEST_106 | GT: 0 | Pred: 0 | TP\n",
      "TEST_107 | GT: 0 | Pred: 0 | TP\n",
      "TEST_108 | GT: 3 | Pred: 3 | TP\n",
      "TEST_109 | GT: 1 | Pred: 1 | TP\n",
      "TEST_110 | GT: 0 | Pred: 0 | TP\n",
      "TEST_111 | GT: 1 | Pred: 1 | TP\n",
      "TEST_112 | GT: 4 | Pred: 4 | TP\n",
      "TEST_113 | GT: 3 | Pred: 3 | TP\n",
      "TEST_114 | GT: 1 | Pred: 1 | TP\n",
      "TEST_115 | GT: 1 | Pred: 1 | TP\n",
      "TEST_116 | GT: 2 | Pred: 2 | TP\n",
      "TEST_117 | GT: 1 | Pred: 1 | TP\n",
      "TEST_118 | GT: 1 | Pred: 1 | TP\n",
      "TEST_119 | GT: 3 | Pred: 3 | TP\n",
      "TEST_120 | GT: 4 | Pred: 3 | FP <-- 틀림\n",
      "TEST_121 | GT: 3 | Pred: 3 | TP\n",
      "TEST_122 | GT: 1 | Pred: 1 | TP\n",
      "TEST_123 | GT: 1 | Pred: 1 | TP\n",
      "TEST_124 | GT: 3 | Pred: 3 | TP\n",
      "TEST_125 | GT: 4 | Pred: 4 | TP\n",
      "TEST_126 | GT: 0 | Pred: 0 | TP\n",
      "TEST_127 | GT: 2 | Pred: 2 | TP\n",
      "TEST_128 | GT: 1 | Pred: 1 | TP\n",
      "TEST_129 | GT: 4 | Pred: 4 | TP\n",
      "TEST_130 | GT: 0 | Pred: 0 | TP\n",
      "TEST_131 | GT: 0 | Pred: 0 | TP\n",
      "TEST_132 | GT: 4 | Pred: 4 | TP\n",
      "TEST_133 | GT: 2 | Pred: 2 | TP\n",
      "TEST_134 | GT: 4 | Pred: 4 | TP\n",
      "TEST_135 | GT: 4 | Pred: 4 | TP\n",
      "TEST_136 | GT: 0 | Pred: 0 | TP\n",
      "TEST_137 | GT: 2 | Pred: 2 | TP\n",
      "TEST_138 | GT: 2 | Pred: 2 | TP\n",
      "TEST_139 | GT: 0 | Pred: 0 | TP\n",
      "TEST_140 | GT: 0 | Pred: 0 | TP\n",
      "TEST_141 | GT: 0 | Pred: 0 | TP\n",
      "TEST_142 | GT: 4 | Pred: 4 | TP\n",
      "TEST_143 | GT: 1 | Pred: 1 | TP\n",
      "TEST_144 | GT: 3 | Pred: 3 | TP\n",
      "TEST_145 | GT: 1 | Pred: 1 | TP\n",
      "TEST_146 | GT: 2 | Pred: 2 | TP\n",
      "TEST_147 | GT: 2 | Pred: 2 | TP\n",
      "TEST_148 | GT: 2 | Pred: 2 | TP\n",
      "TEST_149 | GT: 4 | Pred: 4 | TP\n",
      "TEST_150 | GT: 0 | Pred: 0 | TP\n",
      "TEST_151 | GT: 1 | Pred: 1 | TP\n",
      "TEST_152 | GT: 2 | Pred: 2 | TP\n",
      "Macro F1 Score: 0.9664540138224348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "clear_memory()\n",
    "video_path = './origin test'\n",
    "id_list, label_list = ['id'], ['label']\n",
    "video_list = os.listdir(video_path)\n",
    "video_list = natsort.natsorted(video_list)\n",
    "for video_name in tqdm(video_list):\n",
    "    pred_class = final_model('{}/{}'.format(video_path, video_name))\n",
    "    id_list.append(video_name.split('.')[0])\n",
    "    label_list.append(pred_class)\n",
    "with open('./result.csv', 'w', encoding = 'utf-8-sig') as f:\n",
    "    cnt = 0\n",
    "    for i, j in zip(id_list, label_list):\n",
    "        if cnt == len(id_list) - 1:\n",
    "            f.write('{},{}'.format(i, j))\n",
    "            print('csv 저장 완료')\n",
    "        else:\n",
    "            f.write('{},{}\\n'.format(i, j))\n",
    "        cnt += 1\n",
    "# 정답지 제작\n",
    "answer_sheet = []\n",
    "folder_list = os.listdir('./test_label')\n",
    "for folder in folder_list:\n",
    "    video_list = os.listdir('./test_label/{}'.format(folder))\n",
    "    for video_name in video_list:\n",
    "        answer_sheet.append('{},{}'.format(video_name.split('.')[0], folder))\n",
    "answer_sheet = natsort.natsorted(answer_sheet)\n",
    "# 예측지 불러오기\n",
    "with open('./result.csv', 'r', encoding = 'utf-8-sig') as f:\n",
    "    f = f.read()\n",
    "result = f.split('\\n')\n",
    "del result[0]\n",
    "# 정답지랑 예측지 비교\n",
    "tp_list, fp_list, f1_list = [], [], []\n",
    "for i in range(len(folder_list)):\n",
    "    tp_list.append(0)\n",
    "    fp_list.append(0)\n",
    "for gt, pred in zip(answer_sheet, result):\n",
    "    gt = gt.split(',')\n",
    "    pred = pred.split(',')\n",
    "    if gt[0] == pred[0]:\n",
    "        if gt[1] == pred[1]: \n",
    "            tp_list[int(gt[1])] += 1\n",
    "            status = 'TP'\n",
    "        else: \n",
    "            fp_list[int(gt[1])] += 1\n",
    "            status = 'FP <-- 틀림'\n",
    "    else: print('Sort 여부 안 맞음')\n",
    "    print('{} | GT: {} | Pred: {} | {}'.format(gt[0], gt[1], pred[1], status))\n",
    "# f1 score 계산\n",
    "for tp, fp in zip(tp_list, fp_list):\n",
    "    f1_score = tp / (tp + fp)\n",
    "    f1_list.append(f1_score)\n",
    "macro_f1 = sum(f1_list) / len(f1_list)\n",
    "print('Macro F1 Score: {}'.format(macro_f1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mldl",
   "language": "python",
   "name": "mldl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
